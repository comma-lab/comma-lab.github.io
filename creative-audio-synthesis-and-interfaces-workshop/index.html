<!DOCTYPE HTML>
<!--
	Read Only by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->


<html>
	<head>
		<title>Creative Audio Synthesis and Interfaces Workshop</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link href='https://fonts.googleapis.com/css?family=Inter Tight' rel='stylesheet'>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

							<section id="one">
								<div class="image main" data-position="center">
									<img src="images/casi_workshop_banner.png" alt=""/>
								</div>
								<div class="container">
									<!-- <h2 id="P1" style="margin-bottom: 2em;">Creative Audio Synthesis and Interfaces Workshop</h2> -->
									<ul class="feature-icons">
										<li class="icon solid fa-stopwatch custom-width-left"><h5>WHEN</h5>
										<span style="font-size: 2em; margin-top: 1em; display: block;">15 July 2025</span></li>
										<li class="icon solid fa-question custom-width-right"><h5>WHAT</h5>Talks and performances exploring the intersection of creative audio synthesis and AI-enabled synthesizer programming.</li>
										<li class="icon solid fa-users custom-width-left"><h5>WHO ARE WE</h5>Organised by C4DM's <br><a href="https://comma.eecs.qmul.ac.uk/" target="_blank">Communication Acoustics Lab</a></li>
										<li class="icon solid fa-university custom-width-right"><h5>WHERE</h5>
										G2 & Performance Lab, Engineering Building,<br> QMUL’s <a href="https://www.qmul.ac.uk/about/howtofindus/mileend/index.html" target="_blank">Mile End campus</a>
										<<a href='assets/how-to-reach-us.pdf' target="_blank">access instructions</a>>
										</li>
									</ul>
									* The following information is subject to change and will be finalized in the coming weeks.
									<h3 id="P4">Schedule</h3>
									<h4>@ G2</h4>
									<p><i>9:30</i> – Coffee<br>
									<i>10:00</i> – Workshop Introduction<br>
									<i>10:10</i> - Session 1<br>
									<i>12:40</i> – Lunch<br>
									<i>13:40</i> – Session 2<br>
									<i>15:40</i> – Close Workshop</p>
									<h4>@ Performance Lab</h4>
									<p><i>16:00</i> – Sound Check<br>
									< 17:30 Dinner Break ><br>
									<i>19:30</i> – Evening Concert</p>
									<h3 id="P5">Invited Talks and Performances</h3>
									<ul class="feature-icons">
										<li class="icon solid fa-microphone"><h5>TALKS</h5><br>
										<em><span class="date-font">Title: Facilitating serendipitous sound discoveries with simulations of open-ended evolution</span></em><br>
										<em><span class="date-font">Author: Björn Thor Jónsson -- University of Oslo</span></em><br>
										Abstract: Björn will explore the application of evolutionary algorithms for sound discovery, particularly how Quality Diversity search can facilitate serendipitous discoveries beyond what can be found by prompting models trained on existing data. A specific sound synthesis approach based on pattern-producing networks, coupled with DSP graphs, will also be examined, along with the integration of its neuro-evolution within the evolutionary simulations. Furthermore, the relevance of these sound discoveries in compositional contexts will be addressed, along with efforts to visualise the evolutionary processes and enable interaction with the discovered artefacts.<br><br>
										<em><span class="date-font">Title: Perceptually Aligned Deep Image Sonification</span></em><br>
										<em><span class="date-font">Author: Balint Laczko -- University of Oslo</span></em><br>
										Abstract: Imaging technology has dramatically expanded our understanding of biological systems. This overabundance of images has come with unique problems, such as visual overload, which can potentially obscure data relationships and induce eye fatigue or divert vision from important tasks. Image sonification offers potential solutions to these problems by channeling data into the auditory domain, leveraging our natural pattern recognition skills through hearing. In my PhD project I have been exploring the potential of Machine Learning in solving the two fundamental challenges of image sonification: the perceptually aligned representations of images and sounds, and the cross-modal mapping between them. In this talk I will present my journey through timbre spaces, Differentiable DSP, and cross-modal domain transfer in search of new methods for image sonification.<br><br>
										<em><span class="date-font">Title: Autonomous control of synthesis parameters with listening-based reinforcement learning</span></em><br>
										<em><span class="date-font">Author: Vicenzo Madaghiele -- University of Oslo</span></em><br>
										Abstract: Music improvisation requires listening, reflection, and timely decision-making. When improvising together, musicians make decisions in time, responding to other musicians’ ideas, their style, and the musical context of the performance. I present a novel approach for agent-based autonomous control of synthesis in an improvised music context. The model employs reinforcement learning to control the continuous parameters of a sound synthesizer in response to live audio from a musician. The agent is trained on a corpus of audio files that exemplify the musician’s instrument and stylistic repertoire. During training, the agent listens and adapts to the incoming sound according to a set of perceptual descriptors by continuously adjusting the parameters of the synthesizer it controls, and a reward function expressing a musical objective. To achieve this objective, the agent learns specific strategies that characterize its autonomous behavior in a live interaction. I discuss the theoretical background, the formal model of improvisational interactions, a preliminary implementation of the model and its application in three selected scenarios.<br><br>
										<em><span class="date-font">Title: Timbre latent space transformations for interactive musical systems oriented to timbral music-making.</span></em><br>
										<em><span class="date-font">Author: Andrea Bolzoni -- The Open University</span></em><br>
										Abstract: This talk presents a set of timbre latent space transformations implemented in an interactive musical system for timbral music-making, called TimbReflex. The system enables musicians to explore timbre through turn-taking interactions that structure the musical exchange. By transforming the timbre latent space of the performer’s material, TimbReflex generates sonic variations that elicit further exploration of the musician’s timbral vocabulary. The results show the system's potential to support rich timbral exploration in the context of free improvisation. These findings point toward promising directions for further development and the application of similar methods in interactive, timbre-oriented musical systems.<br><br>
										<em><span class="date-font">Title: Designing Percussive Timbre Remappings: Negotiating Audio Representations and Evolving Parameter Spaces</span></em><br>
										<em><span class="date-font">Author: Jordie Shier -- Queen Mary University of London</span></em><br>
										Abstract: Timbre remapping is an approach to audio-to-synthesizer parameter mapping that aims to transfer timbral expressions from a source instrument onto synthesizer controls. This process is complicated by the ill-defined nature of timbre and the complex relationship between synthesizer parameters and their sonic output. In this work, we focus on real-time timbre remapping with percussion instruments, combining technical development with practice-based methods to address these challenges. As a technical contribution, we introduce a genetic algorithm – applicable to black-box synthesizers including VSTs and modular synthesizers – to generate datasets of synthesizer presets that vary according to target timbres. Additionally, we propose a neural network-based approach to predict control features from short onset windows, enabling low-latency performance and feature-based control. Our technical development is grounded in musical practice, demonstrating how iterative and collaborative processes can yield insights into open-ended challenges in DMI design. Experiments on various audio representations uncover meaningful insights into timbre remapping by coupling data-driven design with practice-based reflection. This work is accompanied by an annotated portfolio, presenting a series of musical performances and experiments with reflections.<br><br>
										<em><span class="date-font">Title: Can a Sound Matching Model Produce Audio Embeddings that Align with Timbre Similarity Rated by Humans?</span></em><br>
										<em><span class="date-font">Author: Haokun Tian -- Queen Mary University of London</span></em><br>
										Abstract: Psychoacoustical so-called “timbre spaces” map perceptual similarity ratings of instrument sounds onto low-dimensional embeddings via multidimensional scaling but suffer from scalability issues and are incapable of generalization. Recent results from audio (music and speech) quality assessment as well as image similarity have shown that deep learning provides emergent embeddings that align well with human perception while being largely free from these constraints. In this talk, we present metrics to evaluate the alignment between three representations—extracted from a simple sound matching model—and the existing "timbre space" data containing 2,614 pairwise ratings on 334 audio samples. Among the tested representations, we highlight the effectiveness of the "style" embeddings, which are inspired by the style transfer task in computer vision. We compare our results to commonly used representations, including MFCCs and CLAP embeddings. Furthermore, based on the performance of the style embedding derived from the CLAP model, we demonstrate its broader effectiveness in modeling human timbre perception.<br><br>
										<em><span class="date-font">Title: GuitarFlow: Realistic Electric Guitar Synthesis From Tablatures via Flow Matching and Style Transfer</span></em><br>
										<em><span class="date-font">Author: Jackson Loth -- Queen Mary University of London</span></em><br>
										Abstract: Music generation in the audio domain using artificial intelligence (AI) has witnessed steady progress in recent years. However for some instruments, particularly the guitar, controllable instrument synthesis remains limited in expressivity. We introduce GuitarFlow, a model with a high degree of controllability designed specifically for electric guitar synthesis. The generative process is guided using tablatures, an ubiquitous and intuitive guitar-specific symbolic format. The tablature format easily represents guitar-specific playing techniques (e.g. bends, muted strings and legatos), which are more difficult to represent in other common music notation formats such as MIDI. Our model relies on an intermediary step of first rendering the tablature to audio using a simple sample-based virtual instrument, then performing style transfer using Flow Matching in order to transform the virtual instrument audio into more realistic sounding examples. This results in a model that is quick to train and to perform inference, requiring less than 6 hours of training data. We present the results of objective evaluation metrics, together with a listening test, in which we show significant improvement in the realism of the generated guitar audio from tablatures.<br>
										</li>
										<li class="icon solid fa-music"><h5>PERFORMANCES</h5>
										<em><span class="date-font">Title: Experience Replay</span></em><br>
										<em><span class="date-font">Author: Vicenzo Madaghiele -- University of Oslo</span></em><br>
										Abstract: Experience Replay is an improvised performance in which autonomous sound-generating processes from various sources are combined into a network of feedback relations focusing on dynamic and textural evolution. The performance employs electric guitar, sound granulation and chaotic synthesis to explore states of fragile equilibrium and apparent disorder.</li>
									</ul>
									<p><center><h4>ORGANISERS</h4><p>
									<p></p>
									<img src="images/partner1.png" alt=" " style="width: 35%; max-height: auto" />
								</div>
							</section>
					</div>

				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<ul class="copyright">
								<li>&copy; 2025 <a href="https://comma.eecs.qmul.ac.uk/" target="_blank">Communication Acoustics Lab</a>, Queen Mary University of London. Theme by <a href="http://html5up.net" target="_blank">HTML5 UP</a>.</li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
