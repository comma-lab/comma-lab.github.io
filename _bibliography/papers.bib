%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Charalampos Saitis at 2022-09-25 11:43:35 +0100 


%% Saved with string encoding Unicode (UTF-8) 

@article{jincheng_preprint_2023_a,
	abbr = {Preprint},
	author = {Zhang, Jincheng and Tang, Jingjing and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
	title = {Composer Style-specific Symbolic Music Generation Using Vector Quantized Discrete Diffusion Models},
	journal = {arXiv},
	abstract = {Emerging Denoising Diffusion Probabilistic Models (DDPM) have become increasingly utilised because of promising results they have achieved in diverse generative tasks with continuous data, such as image and sound synthesis. Nonetheless, the success of diffusion models has not been fully extended to discrete symbolic music. We propose to combine a vector quantized variational autoencoder (VQ-VAE) and discrete diffusion models for the generation of symbolic music with desired composer styles. The trained VQ-VAE can represent symbolic music as a sequence of indexes that correspond to specific entries in a learned codebook. Subsequently, a discrete diffusion model is used to model the VQ-VAE's discrete latent space. The diffusion model is trained to generate intermediate music sequences consisting of codebook indexes, which are then decoded to symbolic music using the VQ-VAE's decoder. The results demonstrate our model can generate symbolic music with target composer styles that meet the given conditions with a high accuracy of 72.36\%.},
	arxiv = {2310.14044},
	year = {2023}}

@article{jincheng_preprint_2023_b,
	abbr = {Preprint},
	author = {Zhang, Jincheng and Fazekas, Gy{\"o}rgy and Saitis, Charalampos},
	title = {Fast Diffusion GAN Model for Symbolic Music Generation Controlled by Emotions},
	journal = {arXiv},
	abstract = {Diffusion models have shown promising results for a wide range of generative tasks with continuous data, such as image and audio synthesis. However, little progress has been made on using diffusion models to generate discrete symbolic music because this new class of generative models are not well suited for discrete data while its iterative sampling process is computationally expensive. In this work, we propose a diffusion model combined with a Generative Adversarial Network, aiming to (i) alleviate one of the remaining challenges in algorithmic music generation which is the control of generation towards a target emotion, and (ii) mitigate the slow sampling drawback of diffusion models applied to symbolic music generation. We first used a trained Variational Autoencoder to obtain embeddings of a symbolic music dataset with emotion labels and then used those to train a diffusion model. Our results demonstrate the successful control of our diffusion model to generate symbolic music with a desired emotion. Our model achieves several orders of magnitude improvement in computational cost, requiring merely four time steps to denoise while the steps required by current state-of-the-art diffusion models for symbolic music generation is in the order of thousands.},
	arxiv = {2310.14040},
	year = {2023}}

@article{ben_DDSPreview_2023,
	abbr = {Preprint},
	author = {Hayes, Ben and Shier, Jordie and Fazekas, Gy{\"o}rgy and McPherson, Andrew and Saitis, Charalampos},
	title = {A Review of Differentiable Digital Signal Processing for Music & Speech Synthesis},
	journal = {arXiv},
	abstract = {The term ``differentiable digital signal processing'' describes a family of techniques in which loss function gradients are backpropagated through digital signal processors, facilitating their integration into neural networks. This article surveys the literature on differentiable audio signal processing, focusing on its use in music & speech synthesis. We catalogue applications to tasks including music performance rendering, sound matching, and voice transformation, discussing the motivations for and implications of the use of this methodology. This is accompanied by an overview of digital signal processing operations that have been implemented differentiably. Finally, we highlight open challenges, including optimisation pathologies, robustness to real-world conditions, and design trade-offs, and discuss directions for future research.},
	arxiv = {2308.15422},
	year = {2023}}

@inproceedings{bleiz_cscw_2023,
	abbr = {CSCW},
	author = {Del Sette, Bleiz Macsen and Carnes, Dawn and Saitis, Charalampos},
	title = {Sound of Care: Towards a Co-Operative AI Digital Pain Companion to Support People with Chronic Primary Pain},
	year = {2023},
	abstract = {This work investigates the role of sound and technology in the everyday lives of people with chronic primary pain. Our primary goal was to inform the first participatory design workshop of Sound of Care, a new eHealth system for pain self-management. We used an ethical stakeholder analysis to inform a round of exploratory interviews, run with 8 participants including people with chronic primary pain, carers, and healthcare workers. We found that sound and technology serve as important but often unstructured tool, helping with distraction, mood regulation and sleep. The experience of pain and musical preferences are highly personal, and communicating or understanding pain can be challenging, even within family members. To address the gaps in current chronic pain self-management care, we propose the use a sound-based AI-driven system, a Digital Pain Companion, using sonification to create a shared decision-making space, enhancing agency over treatment in a co-operative care environment.},
	booktitle = {Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing},
	pages = {283–288},
	pdf = {https://uco.repository.guildhe.ac.uk/id/eprint/199/1/Sound%20of%20Care.pdf},
	location = {Minneapolis, MN, USA}}

@inproceedings{luca_ismir_2023,
	abbr = {ISMIR},
	year = {2023},
	author = {Marinelli, Luca and Fazekas, Gy{\"o}rgy and Saitis, Charalampos},
	title = {Gender-Coded Sound: Analysing the Gendering of Music in Toy Commercials via Multi-Task Learning},
	booktitle = {24th International Society for Music Information Retrieval Conference},
	pdf = {https://www.researchgate.net/profile/Luca-Marinelli-5/publication/372279840_Gender-Coded_Sound_Analysing_the_Gendering_of_Music_in_Toy_Commercials_via_Multi-Task_Learning/links/64ad781295bbbe0c6e2cc1ec/Gender-Coded-Sound-Analysing-the-Gendering-of-Music-in-Toy-Commercials-via-Multi-Task-Learning.pdf},
	abstract = {Music can convey ideological stances, and gender is just one of them. Evidence from musicology and psychology research shows that gender-loaded messages can be reliably encoded and decoded via musical sounds. However, much of this evidence comes from examining music in isolation, while studies of the gendering of music within multimodal communicative events are sparse. In this paper, we outline a method to automatically analyse how music in TV advertising aimed at children may be deliberately used to reinforce traditional gender roles. Our dataset of 606 commercials included music-focused mid-level perceptual features, multimodal aesthetic emotions, and content analytical items. Despite its limited size, and because of the extreme gender polarisation inherent in toy advertisements, we obtained noteworthy results by leveraging multi-task transfer learning on our densely annotated dataset. The models were trained to categorise commercials based on their intended target audience, specifically distinguishing between masculine, feminine, and mixed audiences. Additionally, to provide explainability for the classification in gender targets, the models were jointly trained to perform regressions on emotion ratings across six scales, and on mid-level musical perceptual attributes across twelve scales. Standing in the context of MIR, computational social studies and critical analysis, this study may benefit not only music scholars but also advertisers, policymakers, and broadcasters.},
	video = {https://www.youtube.com/watch?v=d96uEzo6VG4}}

@inproceedings{jordie_FA_2023,
	abbr = {{Forum Acusticum}},
	abstract = {Differentiable digital signal processing (DDSP) techniques, including methods for audio synthesis, have gained attention in recent years and lend themselves to interpretability in the parameter space. However, current differentiable synthesis methods have not explicitly sought to model the transient portion of signals, which is important for percussive sounds. In this work, we present a unified synthesis framework aiming to address transient generation and percussive synthesis within a DDSP framework. To this end, we propose a model for percussive synthesis that builds on sinusoidal modeling synthesis and incorporates a modulated temporal convolutional network for transient generation. We use a modified sinusoidal peak picking algorithm to generate time-varying non-harmonic sinusoids and pair it with differentiable noise and transient encoders that are jointly trained to reconstruct drumset sounds. We compute a set of reconstruction metrics using a large dataset of acoustic and electronic percussion samples that show that our method leads to improved onset signal reconstruction for membranophone percussion instruments.},
	address = {Turin, Italy},
	author = {Shier, Jordie and Caspe, Franco and Robertson, Andrew and Sandler, Mark and Saitis, Charalampos and McPherson, Andrew},
	booktitle = {{10th Convention of the European Acoustics Association}},
	pdf = {https://jordieshier.com/assets/pdf/shier2023differentiable_paper.pdf},
	title = {Differentiable Modelling of Percussive Audio with Transient and Spectral Synthesis},
	year = {2023}}

@inproceedings{ben_iclr_2023,
	abbr = {ICLR},
	author = {Hayes, Ben and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
	title = {The Responsibility Problem in Neural Networks with Unordered Targets},
	booktitle = {11th International Conference on Learning Representations, Tiny Papers},
	abstract = {We discuss the discontinuities that arise when mapping unordered objects to neural network outputs of fixed permutation, referred to as the responsibility problem. Prior work has proved the existence of the issue by identifying a single discontinuity. Here, we show that discontinuities under such models are uncountably infinite, motivating further research into neural networks for unordered data.},
	arxiv = {2304.09499},
	year = {2023}}
	
@inproceedings{rodrigo_nime_2023,
	abbr = {NIME},
	author = {Diaz, Rodrigo and Saitis, Charalampos and Sandler, Mark},
	title = {Interactive Neural Resonators},
	booktitle = {International Conference on New Interfaces for Musical Expression},
	abstract = {In this work, we propose a method for the controllable synthesis of real-time contact sounds using neural resonators. Previous works have used physically inspired statistical methods and physical modelling for object materials and excitation signals. Our method incorporates differentiable second-order resonators and estimates their coefficients using a neural network that is conditioned on physical parameters. This allows for interactive dynamic control and the generation of novel sounds in an intuitive manner. We demonstrate the practical implementation of our method and explore its potential creative applications.},
	arxiv = {2305.14867},
	video = {https://interactive-neural-resonators.com/},
	year = {2023}}

@inproceedings{vjosa_ic2s2_2023,
	abbr = {IC2S2},
	author = {Preniqi, Vjosa and Kalimeri, Kyriaki and Kaltenbrunner, Andreas and Saitis, Charalampos},
	booktitle = {9th International Conference on Computational Social Science},
	title = {Gender differences in Moral Valence, Sentiment, and Narratives of Song Lyrics Over Time},
	abstract = {vjosa_IC2S2_23.pdf},
	year = {2023}}
	
@inproceedings{luca_icmpc_2023,
	abbr = {ICMPC},
	year = {2023},
	author = {Marinelli, Luca and Saitis, Charalampos},
	title = {Analysing the Gendering of Music in Toy Commercials via Mid-level Perceptual Features},
	booktitle = {17th International Conference on Music Perception and Cognition},
	abstract = {Luca_ICMPC23.pdf}}

@inproceedings{vjosa_icmpc_2023a,
	abbr = {ICMPC},
	year = {2023},
	author = {Preniqi, Vjosa and Kalimeri, Kyriaki and Saitis, Charalampos},
	title = {Exploring the Role of Audio and Lyrics in Explaining Moral Worldviews},
	booktitle = {17th International Conference on Music Perception and Cognition},
	abstract = {vjosa_ICMPC_23_1.pdf}}
	
@inproceedings{vjosa_icmpc_2023b,
	abbr = {ICMPC},
	year = {2023},
	author = {Preniqi, Vjosa and Kalimeri, Kyriaki and Kaltenbrunner, Andreas and Saitis, Charalampos},
	title = {Evolution of Moral Valence in Lyrics Over Time},
	booktitle = {17th International Conference on Music Perception and Cognition},
	abstract = {vjosa_ICMPC_23_2.pdf}}
	
@inproceedings{charis_cgpt_timbre,
	abbr = {Timbre},
	year = {2023},
	author = {Saitis, Charalampos and Siedenburg, Kai},
	title = {When ChatGPT Talks Timbre},
	booktitle = {3rd International Conference on Timbre},
	abstract = {3603_cam_ready.pdf}}

@inproceedings{charis_nime_timbre,
	abbr = {Timbre},
	year = {2023},
	author = {Saitis, Charalampos and Torshizi, Maryam F. and Preniqi, Vjosa and Del Sette, Bleiz M. and Fazekas, György},
	title = {When NIME and ISMIR Talk Timbre},
	booktitle = {3rd International Conference on Timbre},
	poster = {Saitis_timbre2023.pdf},
	abstract = {553_camera_ready.pdf}}

@article{charis_cgpt_arXiv,
	abbr = {Preprint},
	abstract = {Semantic dimensions of sound have been playing a central role in understanding the nature of auditory sensory experience as well as the broader relation between perception, language, and meaning. Accordingly, and given the recent proliferation of large language models (LLMs), here we asked whether such models exhibit an organisation of perceptual semantics similar to those observed in humans. Specifically, we prompted ChatGPT, a chatbot based on a state-of-the-art LLM, to rate musical instrument sounds on a set of 20 semantic scales. We elicited multiple responses in separate chats, analogous to having multiple human raters. ChatGPT generated semantic profiles that only partially correlated with human ratings, yet showed robust agreement along well-known psychophysical dimensions of musical sounds such as brightness (bright-dark) and pitch height (deep-high). Exploratory factor analysis suggested the same dimensionality but different spatial configuration of a latent factor space between the chatbot and human ratings. Unexpectedly, the chatbot showed degrees of internal variability that were comparable in magnitude to that of human ratings. Our work highlights the potential of LLMs to capture salient dimensions of human sensory experience.},
	author = {Siedenburg, Kai and Saitis, Charalampos},
	journal = {arXiv},
	arxiv = {2304.07830},
	title = {The language of sounds unheard: Exploring musical timbre semantics of large language models},
	year = {2023}}

@article{vjosa_PsyArXiv,
	abbr = {Preprint},
	abstract = {Music is a fundamental element in every culture, serving as a universal means of expressing our emotions, feelings, and beliefs. This work investigates the link between our moral values and musical choices through lyrics and audio analyses. We align the psychometric scores of 1,480 participants to acoustics and lyrics features obtained from the top 5 songs of their preferred music artists from Facebook Page Likes. We employ a variety of lyric text processing techniques, including lexicon-based approaches and BERT-based embeddings, to identify each song's narrative, moral valence, attitude, and emotions. In addition, we extract both low- and high-level audio features to comprehend the encoded information in participants' musical choices and improve the moral inferences. We propose a Machine Learning approach and assess the predictive power of lyrical and acoustic features separately and in a multimodal framework for predicting moral values. Results indicate that lyrics and audio features from the artists people like inform us about their morality. Though the most predictive features vary per moral value, the models that utilised a combination of lyrics and audio characteristics were the most successful in predicting moral values, outperforming the models that only used basic features such as user demographics, the popularity of the artists, and the number of likes per user. Audio features boosted the accuracy in the prediction of empathy and equality compared to textual features, while the opposite happened for hierarchy and tradition, where higher prediction scores were driven by lyrical features. This demonstrates the importance of both lyrics and audio features in capturing moral values. The insights gained from our study have a broad range of potential uses, including customising the music experience to meet individual needs, music rehabilitation, or even effective communication campaign crafting.},
	author = {Preniqi, Vjosa and Kalimeri, Kyriaki and Saitis, Charalampos},
	journal = {PsyArXiv},
	psyarxiv = {xeuty},
	title = {Soundscapes of morality: Linking music preferences and moral values through lyrics and audio},
	year = {2023}}

@inproceedings{ben_icassp_2023,
	abbr = {ICASSP},
	year = {2023},
	author = {Hayes, Ben and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
	arxiv = {2210.14476},
	pdf = {Sinusoidal_Frequency_Estimation_by_Gradient_Descent.pdf},
	title = {Sinusoidal Frequency Estimation by Gradient Descent},
	booktitle = {48th IEEE International Conference on Acoustics, Speech and Signal Processing},
	abstract = {Sinusoidal parameter estimation is a fundamental task in applications from spectral analysis to time-series forecasting. Estimating the sinusoidal frequency parameter by gradient descent is, however, often impossible as the error function is non-convex and densely populated with local minima. The growing family of differentiable signal processing methods has therefore been unable to tune the frequency of oscillatory components, preventing their use in a broad range of applications. This work presents a technique for joint sinusoidal frequency and amplitude estimation using the Wirtinger derivatives of a complex exponential surrogate and any first order gradient-based optimiser, enabling end-to-end training of neural network controllers for unconstrained sinusoidal models.}}
	
@inproceedings{rodrigo_icassp_2023,
	abbr = {ICASSP},
	year = {2023},
	author = {Diaz, Rodrigo and Hayes, Ben and Saitis, Charalampos and Fazekas, Gy{\"o}rgy and Sandler, Mark},
	arxiv = {2210.15306},
	pdf = {Rigid-Body_Sound_Synthesis_with_Differentiable_Modal_Resonators.pdf},
	title = {Rigid-Body Sound Synthesis with Differentiable Modal Resonators},
	booktitle = {48th IEEE International Conference on Acoustics, Speech and Signal Processing},
	abstract = {Physical models of rigid bodies are used for sound synthesis in applications from virtual environments to music production. Traditional methods, such as modal synthesis, often rely on computationally expensive numerical solvers, while recent deep learning approaches are limited by post-processing of their results. In this work, we present a novel end-to-end framework for training a deep neural network to generate modal resonators for a given 2D shape and material using a bank of differentiable IIR filters. We demonstrate our method on a dataset of synthetic objects but train our model using an audio-domain objective, paving the way for physically-informed synthesisers to be learned directly from recordings of real-world objects.}}

@article{charis_musperc_2023,
	abbr = {Music Perception},
	author = {Reymore, Lindsey and Noble, Jason and Saitis, Charalampos and Traube, Caroline and Wallmark, Zachary},
	journal = {Music Perception},
	year = {2023},
	volume = {40},
	pages = {253--274},
	number = {3},
	title = {Timbre semantic associations vary both between and within instruments: An empirical study incorporating register and pitch height},
	pdf = {https://www.mcgill.ca/mpcl/files/mpcl/reymore_2023_muspercept.pdf},
	abstract = {The main objective of this study is to understand how timbre semantic associations — for example, a sound’s timbre perceived as bright, rough, or hollow — vary with register and pitch height across instruments. In this experiment, 540 online participants rated single, sustained notes from eight Western orchestral instruments (flute, oboe, bass clarinet, trumpet, trombone, violin, cello, and vibraphone) across three registers (low, medium, and high) on 20 semantic scales derived from Reymore and Huron (2020). The 24 two-second stimuli, equalized in loudness, were produced using the Vienna Symphonic Library. Exploratory modeling examined relationships between mean ratings of each semantic dimension and instrument, register, and participant musician identity (‘‘musician’’ vs. ‘‘nonmusician’’). For most semantic descriptors, both register and instrument were significant predictors, though the amount of variance explained differed (marginal R^2). Terms that had the strongest positive relationships with register include shrill/harsh/noisy, sparkling/brilliant/bright, ringing/long decay, and percussive. Terms with the strongest negative relationships with register include deep/thick/heavy, raspy/grainy/gravelly, hollow, and woody. Post hoc modeling using only pitch height and only register to predict mean semantic rating suggests that pitch height may explain more variance than does register. Results help clarify the influence of both instrument and relative register (and pitch height) on common timbre semantic associations.}}

@inproceedings{jordie_dmrn_2022,
	abbr = {DMRN},
	year = {2022},
	poster = {https://jordieshier.com/assets/pdf/shier2022dmrn_poster.pdf},
	author = {Shier, Jordie},
	title = {Real-time timbre mapping for synthesized percussive performance},
	booktitle = {{{DMRN}}+17: {{Digital Music Research Network One}}-Day {{Workshop}}},
	selected = {false}}
	
@inproceedings{actor_icmpc_2021,
	abbr = {ICMPC},
	year = {2021},
	author = {Reymore, Lindsey and Noble, Jason and Saitis, Charalampos and Traube, Caroline and Wallmark, Zachary},
	title = {Mapping the semantics of timbre across pitch registers},
	booktitle = {16th International Conference on Music Perception and Cognition},
	abstract = {https://drive.google.com/file/d/1oax1QfdTgmP5bWZF7aOoNyLbm5qFQsex/view},
	video = {https://www.youtube.com/watch?v=c4xJSSB5QVs}}

@inproceedings{ben_sound_instruments_2020,
	abbr = {Talk},
	year = {2020},
	author = {Hayes, Ben and Saitis, Charalampos},
	title = {How we talk about sound: Semantic dimensions of abstract timbres},
	booktitle = {Sound Instruments and Sonic Cultures: An Interdisciplinary Conference},
	note = {National Science & Media Museum},
	abstract = {Synthesisers, in their many forms, enable the realisation of almost any conceivable sound. Their fine-grained control and broad timbral palette call for a descriptive lexicon to enable their verbal differentiation and discussion. While acoustic instruments of the western classical lineage are the subject of an extensive body of enquiry into the perceptual attributes and semantic associations of the sounds they produce, abstract electronic sounds have been comparatively understudied in this regard. In particular, the diverse vocabulary used to describe such classical acoustic instruments can be summarised with three conceptual metaphors—such musical tones have luminance, texture, and mass—but this has yet to be explicitly confirmed for the kinds of electronic sounds that pervade many modern sonic cultures. In this work, we present an experimental paradigm for studying the semantic associations of synthesised sounds, wherein a group of experienced music producers and sound designers interacted with a web-based synthesiser in response to descriptive prompts, and provided comparative semantic ratings on the sounds they created. The words used for semantic ratings were selected by mining a text corpus from the popular modular synthesis forum Muff Wiggler, and analysing the frequency of adjectives in contexts pertaining to timbre. The ratings provided by participants were subject to statistical analysis. From 27 initial adjectives, two underlying semantic factors were revealed: terms including aggressive, hard, and complex associated with the first, and dark and warm with the second. These factors differ from those found for classical acoustic sounds, implying a relationship between the qualia of a sonic experience and the language employed to talk about it. Such insight has implications for how sound is conceptualised, understood, and received within sonic cultures—in particular, those predicated on electronic or abstract sound—and applications in developing novel control schemes for synthesis methods.}}

@inproceedings{max_eurohaptics_2020,
	abbr = {EuroHaptics},
	year = {2020},
	author = {Weber, Maximilian and Saitis, Charalampos},
	title = {Analysing and countering bodily interference in vibrotactile devices introduced by human interaction and physiology},
	booktitle = {12th EuroHaptics Conference},
	abstract = {}}

@inproceedings{luca_dmrn_2019,
	abbr = {DMRN},
	year = {2019},
	abstract = {luca_dmrn_2019.pdf},
	author = {Marinelli, Luca and Lykartsis, Athanasios and Saitis, Charalampos},
	title = {Modulation Spectra for Musical Dynamics Perception and Retrieval},
	booktitle = {{{DMRN}}+14: {{Digital Music Research Network One}}-Day {{Workshop}}},
	selected = {false}}

@inproceedings{charis_ica_2019,
	abbr = {ICA},
	year = {2019},
	abstract = {http://pub.dega-akustik.de/ICA2019/data/articles/000813.pdf},
	author = {Saitis, Charalampos and Siedenburg, Kai and Schuladen, Paul and Reuter, Christoph},
	title = {The role of attack transients in timbral brightness perception},
	booktitle = {23rd International Congress on Acoustics},
	selected = {false}}

@inproceedings{charis_smpc_2019b,
	abbr = {SMPC},
	year = {2019},
	abstract = {Brightness has been long shown to play a major role in timbre perception but relatively little is known about the specific acoustic and cognitive factors that affect brightness ratings of musical instrument sounds. Previous work indicated that sound source categories influence general timbre dissimilarity ratings. To examine whether source categories also exert an effect on brightness ratings of timbre, we collected brightness dissimilarity ratings of 14 orchestral instrument tones from 40 musically experienced listeners and the data were modeled using a partial least-squares regression model that takes audio descriptors of timbre as regressors. It was found that adding predictors derived from sound source categories did not improve the model fit, indicating that timbral brightness is informed mainly by continuously varying properties of the acoustic signal. A multidimensional scaling analysis suggested at least two salient cues: spectral energy distribution and attack time and/or asynchrony in the rise of harmonics. This finding seems to challenge the typical approach of seeking acoustical correlates of brightness in the spectral envelope of the steady-state portion of sounds. To further investigate these aspects in timbral brightness perception, a new group of 40 musically experienced listeners will perform MUSHRA-like brightness ratings of an expanded set of 24 orchestral instrument notes. The goal is to obtain a perceptual scaling of the attribute across a larger set of sounds to help delineate the acoustic ingredients of this important aspect of timbre perception. Preliminary results indicate that between sounds with very close spectral centroid values but different attack times, those with faster attacks tend to be perceived as brighter. Overall, these experiments help clarify the relation between two salient dimensions of timbre: onset and spectral energy distribution.},
	author = {Saitis, Charalampos and Siedenburg, Kai and Reuter, Christoph},
	title = {Revisiting timbral brightness perception},
	booktitle = {Biennial Meeting of the Society for Music Perception and Cognition},
	poster = {Brightness_SMPC_DGM_2019_v2.pdf}}

@inproceedings{charis_smpc_2019a,
	abbr = {SMPC},
	year = {2019},
	abstract = {Imagine listening to the famous soprano Maria Callas (1923–1977) singing the aria “Vissi d’arte” from Puccini’s Tosca. How would you describe the quality of her voice? When describing the timbre of musical sounds, listeners use descriptions such as bright, heavy, round, and rough, among others. In 1890, Stumpf theorized that this diverse vocabulary can be summarized, on the basis of semantic proximities, by three pairs of opposites: dark–bright, soft–rough, and full–empty. Empirical findings across many semantic differential studies from the late 1950s until today have generally confirmed that these are the salient dimensions of timbre semantics. However, most prior work has considered only orchestral instruments, with relatively little attention given to sung tones. At the same time, research on the perception of singing voice quality has primarily focused on verbal attributes associated with phonation type, voice classification, vocal register, vowel intelligibility, and vibrato. Descriptions like pressed, soprano, falsetto, hoarse, or wobble, albeit in themselves a type of timbre semantics, are essentially sound source identifiers acting as semantic descriptors. It remains an open question as to whether the timbral attributes of sung tones, that is verbal attributes that bear no source associations, can be described adequately on the basis of the bright-rough-full semantic space. We present a meta-analysis of previous research on verbal attributes of singing voice timbre that covers not only pedagogical texts but also work from music cognition, psychoacoustics, music information retrieval, musicology, and ethnomusicology. The meta-analysis lays the groundwork for a semantic differential study of sung sounds, providing a more appropriate lexicon on which to draw than simply using verbal scales from related work on instrumental timbre. The meta-analysis will be complemented by a psycholinguistic analysis of free verbalizations provided by singing teachers in a listening test and an acoustic analysis of the tested stimuli.},
	author = {Saitis, Charalampos and Devaney, Johanna},
	title = {There’s more to timbre than musical instruments: a meta-analysis of timbre semantics in singing voice quality perception},
	booktitle = {Biennial Meeting of the Society for Music Perception and Cognition},
	poster = {https://osf.io/an57w}}

@inproceedings{charis_smpc_2019c,
	abbr = {SMPC},
	year = {2019},
	abstract = {Timbre is often described as a complex set of sound features that are not accounted for by pitch, loudness, duration, spatial location, and the acoustic environment. Musical dynamics refers to the perceived or intended loudness of a played note, instructed in music notation as piano or forte (soft or loud) with different dynamic gradations between and beyond. Recent research has shown that even if no loudness cues are available, listeners can still quite reliably identify the intended dynamic strength of a performed sound by relying on timbral features. More recently, acoustical analyses across an extensive set of anechoic recordings of orchestral instrument notes played at pianissimo (pp) and fortissimo (ff) showed that attack slope, spectral skewness, and spectral flatness together explained 72% of the variance in dynamic strength across all instruments, and 89% with an instrument-specific model. Here, we further investigate the role of timbre in musical dynamics, focusing specifically on the contribution of spectral and temporal modulations. Loudness-normalized modulation power spectra (MPS) were used as input representation for a convolutional neural network (CNN). Through visualization of the pp and ff saliency maps of the CNN it was possible to identify discriminant regions of the MPS and define a novel task-specific scalar audio descriptor. A linear discriminant analysis with 10-fold cross-validation using this new MPS-based descriptor on the entire dataset performed better than using the two spectral descriptors (27% error rate reduction). Overall, audio descriptors based on different regions of the MPS could serve as sound representation for machine listening applications, as well as to better delineate the acoustic ingredients of different aspects of auditory perception.},
	author = {Saitis, Charalampos and Marinelli, Luca and Lykartsis, Athanasios and Weinzierl, Stefan},
	title = {Spectrotemporal modulation timbre cues in musical dynamics},
	booktitle = {Biennial Meeting of the Society for Music Perception and Cognition}}

@article{saitis2020timbre,
	abbr = {{Acoust. Sci. Tech.}},
	abstract = {This position paper argues that a systematic study of the behavioral and neural mechanisms of crossmodal correspondences between timbral dimensions of sound and perceptual dimensions of other sensory modalities, such as brightness, roughness, or sweetness, can offer a new way of addressing old questions about the perceptual and neurocognitive mechanisms of auditory semantics. At the same time, timbre and the crossmodal metaphors that dominate its conceptualization can provide a test case for better understanding the neural basis of crossmodal correspondences and human semantic processing in general.},
	author = {Saitis, Charalampos and Weinzierl, Stefan and von Kriegstein, Katharina and Ystad, S{\o}lvi and Cuskley, Christine},
	journal = {Acoustical Science and Technology},
	number = {1},
	pages = {365--368},
	pdf = {charis_AST_2020.pdf},
	publisher = {ACOUSTICAL SOCIETY OF JAPAN},
	title = {Timbre semantics through the lens of crossmodal correspondences: A new way of asking old questions},
	volume = {41},
	year = {2020}}

@article{saitis2021multimodal,
	abbr = {IEEE TAC},
	abstract = {In this study, we aim to better understand the cognitive-emotional experience of visually impaired people when navigating in unfamiliar urban environments, both outdoor and indoor. We propose a multimodal framework based on random forest classifiers, which predict the actual environment among predefined generic classes of urban settings, inferring on real-time, non-invasive, ambulatory monitoring of brain and peripheral biosignals. Model performance reached 93% for the outdoor and 87% for the indoor environments (expressed in weighted AUROC), demonstrating the potential of the approach. Estimating the density distributions of the most predictive biomarkers, we present a series of geographic and temporal visualizations depicting the environmental contexts in which the most intense affective and cognitive reactions take place. A linear mixed model analysis revealed significant differences between categories of vision impairment, but not between normal and impaired vision. Despite the limited size of our cohort, these findings pave the way to emotionally intelligent mobility-enhancing systems, capable of implicit adaptation not only to changing environments but also to shifts in the affective state of the user in relation to different environmental and situational factors.},
	author = {Saitis, Charalampos and Kalimeri, Kyriaki},
	journal = {IEEE Transactions on Affective Computing},
	number = {01},
	pages = {203--214},
	pdf = {https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/59824/Saitis%20Multimodal%20Classification%20of%20Stressful%202019%20Accepted.pdf?sequence=2},
	publisher = {IEEE Computer Society},
	title = {Multimodal Classification of Stressful Environments in Visually Impaired Mobility Using EEG and Peripheral Biosignals},
	volume = {12},
	year = {2021}}

@article{senses_survey,
	abbr = {Preprint},
	abstract = {Academic disciplines spanning cognitive science, art, and music have made strides in understanding how humans sense and experience the world. We now have a better scientific understanding of how human sensation and perception function both in the brain and in interaction than ever before. However, there is little research on how this high level scientific understanding is translated into knowledge for the public more widely. We present descriptive results from a simple survey and compare how public understanding and perception of sensory experience lines up with scientific understanding. Results show that even in a sample with fairly high educational attainment, many respondents were unaware of fairly common forms of sensory variation. In line with the well-documented under representation of sign languages within linguistics, respondents tended to under-estimate the number of sign languages in the world. We outline how our results represent gaps in public understanding of sensory variation, and argue that filling these gaps can form an important early intervention, acting as a basic foundation for improving acceptance, inclusivity, and accessibility for cognitively diverse populations.},
	author = {Cuskley, Christine and Saitis, Charalampos},
	journal = {PsyArXiv},
	psyarxiv = {ghcxv},
	title = {What do people know about sensation and perception? Understanding perceptions of sensory experience},
	year = {2020}}

@inproceedings{marentakis:hal-03234063,
	abbr = {{Forum Acusticum}},
	abstract = {The psychoacoustic investigation of timbre traditionally relies on audio descriptors extracted from anechoic or semi-anechoic recordings of musical instrument sounds, which are presented to listeners in diotic fashion. As a result, the extent to which spectral modifications due to the outer ear interact with timbre perception is not fully understood. As a first step towards investigating this research question, we examine here whether timbre descriptors calculated using HRTF filtered instrumental sounds deviate across ears and from values obtained from the same sounds without HRTF filtering for different listeners. The sound set comprised isolated notes played at the same fundamental frequency and dynamic from a database of anechoic recordings of modern orchestral instruments and some of their classical and baroque precursors. These were convolved with anechoic high spatial resolution HRTFs of human listeners. We present results and discuss implications for research on timbre perception and cognition.},
	address = {Lyon, France},
	author = {Marentakis, Georgios and Saitis, Charalampos},
	booktitle = {{Forum Acusticum}},
	date-modified = {2022-09-25 11:43:31 +0100},
	doi = {10.48465/fa.2020.0992},
	month = Dec,
	pages = {1047-1052},
	pdf = {https://hal.archives-ouvertes.fr/hal-03234063/file/000992.pdf},
	title = {{Timbre in Binaural Listening: A Comparison of Timbre Descriptors in Anechoic and HRTF Filtered Orchestral Sounds}},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.48465/fa.2020.0992}}

@inproceedings{vjosa_ismir_2022,
	abbr = {ISMIR},
	abstract = {This study explores the association between music preferences and moral values by applying text analysis techniques to lyrics. Harvesting data from a Facebook-hosted application, we align psychometric scores of 1,386 users to lyrics from the top 5 songs of their preferred music artists as emerged from Facebook Page Likes. We extract a set of lyrical features related to each song's overarching narrative, moral valence, sentiment, and emotion. A machine learning framework was designed to exploit regression approaches and evaluate the predictive power of lyrical features for inferring moral values. Results suggest that lyrics from top songs of artists people like inform their morality. Virtues of hierarchy and tradition achieve higher prediction scores (between .20 and .30) than values of empathy and equality (between .08 and .11), while basic demographic variables only account for a small part in the models' explainability. This shows the importance of music listening behaviours, as assessed via lyrical preferences, alone in capturing moral values. We discuss the technological and musicological implications and possible future improvements.},
	address = {{Online}},
	pdf = {https://archives.ismir.net/ismir2022/paper/000096.pdf},
	author = {Preniqi, Vjosa and Kalimeri, Kyriaki and Saitis, Charalampos},
	booktitle = {23rd International Society for Music Information Retrieval Conference},
	selected = {true},
	video = {https://drive.google.com/file/d/1MMI5RbZdgjhQOavn7Slxzysb2RQ_Tza1/view},
	title = {More Than Words: Linking Music Preferences and Moral Values Through Lyrics},
	year = {2022}}

@inproceedings{vjosa_cmmr_2021,
	abbr = {CMMR},
	abstract = {Music has always been an integral part of our everyday lives through which we express feelings, emotions, and concepts. Here, we explore the association between music genres, demographics and moral values employing data from an ad-hoc online survey and the Music Learning Histories Dataset. To further characterise the music preferences of the participants the generalist/specialist (GS) score employed. We exploit both classification and regression approaches to assess the predictive power of music preferences for the prediction of demographic attributes as well as the moral values of the participants. Our findings point out that moral values are hard to predict (.62 AUROC_avg) solely by the music listening behaviours, while if basic sociodemographic information is provided the prediction score rises to 4% on average (.66 AUROC_avg), with the Purity foundation to be the one that is steadily the one with the highest accuracy scores. Similar results are obtained from the regression analysis. Finally, we provide with insights on the most predictive music behaviours associated with each moral value that can inform a wide range of applications from rehabilitation practices to communication campaign design.},
	address = {{Online}},
	author = {Preniqi, Vjosa and Kalimeri, Kyriaki and Saitis, Charalampos},
	booktitle = {15th International Symposium on Computer Music Multidisciplinary Research},
	pdf = {https://cmmr2021.github.io/proceedings/pdffiles/cmmr2021_08.pdf},
	selected = {false},
	title = {Modelling Moral Traits with Music Listening Preferences and Demographics},
	year = {2021}}

@inproceedings{charis_cmmr_2019,
	abbr = {CMMR},
	abstract = {This position paper argues that a systematic study of crossmodal correspondences between timbral dimensions of sound and perceptual dimensions of other sensory modalities (e.g., brightness, fullness, roughness, sweetness) can offer a new way of addressing old questions about the perceptual and cognitive mechanisms of timbre semantics, while the latter can provide a test case for better understanding crossmodal correspondences and human semantic processing in general. Furthermore, a systematic investigation of auditory-nonauditory crossmodal correspondences necessitates auditory stimuli that can be intuitively controlled along intrinsic continuous dimensions of timbre, and the collection of behavioural data from appropriate tasks that extend beyond the semantic differential paradigm.},
	address = {{Marseile, France}},
	author = {Saitis, Charalampos},
	booktitle = {14th International Symposium on Computer Music Multidisciplinary Research},
	pdf = {https://hal.archives-ouvertes.fr/hal-02382500/document#page=344},
	selected = {false},
	title = {Beyond the semantic differential: Timbre semantics as crossmodal correspondences},
	year = {2019}}

@inproceedings{charis_isma_2019,
	abbr = {ISMA},
	abstract = {Results from a previous study on the perceptual evaluation of violins that involved playing-based semantic ratings showed that preference for a violin was strongly associated with its perceived sound richness. However, both preference and richness ratings varied widely between individual violinists, likely because musicians conceptualize the same attribute in different ways. To better understand how richness is conceptualized by violinists and how it contributes to the perceived quality of a violin, we analyzed free verbal descriptions collected during a carefully controlled playing task (involving 16 violinists) and in an online survey where no sound examples or other contextual information was present (involving 34 violinists). The analysis was based on a psycholinguistic method, whereby semantic categories are inferred from the verbal data itself through syntactic context and linguistic markers. The main sensory property related to violin sound richness was expressed through words such as full, complex, and dense versus thin and small, referring to the perceived number of partials present in the sound. Another sensory property was expressed through words such as warm, velvety, and smooth versus strident, harsh, and tinny, alluding to spectral energy distribution cues. Haptic cues were also implicated in the conceptualization of violin sound richness.},
	address = {{Marseile, France}},
	author = {Saitis, Charalampos and Fritz, Claudia and Scavone, Gary},
	booktitle = {International Symposium on Musical Acoustics},
	pdf = {charis_ISMA_2019.pdf},
	selected = {false},
	title = {Sounds like melted chocolate: how musicians conceptualize violin sound richness},
	year = {2019}}

@inproceedings{ben_ica_2022,
	abbr = {ICA},
	abstract = {We present timbre.fun, a web-based gamified interactive system where users create sounds in response to semantic prompts (e.g., bright, rough) through exploring a two-dimensional control space that maps nonlinearly to the parameters of a simple hybrid wavetable and amplitude-modulation synthesizer. The current version features 25 semantic adjectives mined from a popular synthesis forum. As well as creating sounds, users can explore heatmaps generated from others' responses, and fit a classifier (k-nearest neighbors) in-browser. timbre.fun is based on recent work, including by the authors, which studied timbre semantic associations through prompted synthesis paradigms. The interactive is embedded in a digital exhibition on sensory variation and interaction (seeingmusic.app) which debuted at the 2021 Edinburgh Science Festival, where it was visited by 197 users from 21 countries over 16 days. As it continues running online, a further 596 visitors from 35 countries have engaged. To date 579 sounds have been created and tagged, which will facilitate parallel research in timbre semantics and neural audio synthesis. Future work will include further gamifying the data collection pipeline, including leveling-up to unlock new words and synthesizers, and a full open-source release.},
	address = {{Gyeongju, Korea}},
	author = {Hayes, Ben and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
	booktitle = {24th International Congress on Acoustics},
	pdf = {ICA_2022_template_final_ABS-0997.pdf},
	selected = {false},
	title = {timbre.fun: A gamified interactive system for crowdsourcing a timbre semantic vocabulary},
	year = {2022}}

@inproceedings{charis_imrf_2022,
	abbr = {IMRF},
	abstract = {Our recent research has shown that people lack knowledge about how the senses interact and are unaware of many common forms of sensory and perceptual variation. We present Seeing Music, a digital interactive exhibition and audiovisual game that translates high-level scientific understanding of sensory variation and cross-modality into knowledge for the public. Using a narrative-driven gamified approach, players are tasked with communicating human music to an extraterrestrial intelligence through visual shape, color and texture using two-dimensional selector panels. Music snippets (12--24 s long) are played continuously in a loop, taken from three custom instrumental compositions designed to vary systematically in terms of timbre, melody, and rhythm. Players can ``level-up'' to unlock new visual features and musical snippets, and explore and evaluate collaborative visualizations made by others. Outside the game, a series of interactive slideshows help visitors learn more about sensory experience, sensory diversity, and how our senses make us human. The exhibition debuted at the 2021 Edinburgh Science Festival, where it was visited by 197 users coming from 21 countries (134 visitors from the UK) over 16 days. As it continues running online, a further 596 visitors from 35 countries (164 from the UK) have engaged. To date, 169 players of Seeing Music have produced more than 42,500 audiovisual mapping datapoints for scientific research purposes. Preliminary analysis suggests that music with less high-frequency energy was mapped to less complex and rounder shapes, bluer and less bright hues, and less dense textures. These trends confirm auditory-visual correspondences previously reported in more controlled laboratory studies, while also offering new insight into how different auditory-visual associations interact with each other. Future work includes improving user motivation and interaction, refining data collection, a full open-source release, and adding new games and informational material about research on the senses.},
	address = {{Online}},
	author = {Saitis, Charalampos and Cuskley, Christine and L{\"o}bbers, Sebastian},
	booktitle = {20th International Multisensory Research Forum},
	selected = {false},
	title = {Seeing Music: Leveraging citizen science and gamification to study cross-sensory associations},
	year = {2022}}

@inproceedings{luca_euromedia_2022,
	abbr = {EuroMedia},
	abstract = {As evidenced by a large body of literature, the gender-stereotyped nature of toy adverts has been widely scrutinised. However, little work has been done in examining the affective impact of these commercials on the audience. It has been proven that repeated exposure to gender-stereotyped messages has the capacity to influence behaviours, beliefs and attitudes. In particular, media can influence emotion socialization, and gender differences in emotion expression might emerge (Scherr 2018). In this study, we investigated whether commercials elicit emotions at different intensities with respect to the gender of their target audience. Furthermore, we evaluated whether such emotions follow distinct underlying latent structures. A total of 1081 ratings of 10 unipolar aesthetic emotion scales were collected for 135 commercials (45 for each masculine, feminine, and mixed target audience) from 80 UK nationals (35 F, 45 M) aged 18 to 76. The main reason for collecting our ratings from adults was that, already by age 11, children exhibit adult-like emotion recognition capabilities (Hunter 2011). Seven scales showed significant differences between commercials for distinct audiences; with five, in particular, revealing a strong polarization (happiness, amusement, beauty, calm, and anger). In addition, parallel analysis showed that a minimum of three factors are needed to explain the ratings for masculine and mixed targeted commercials, while only two are needed for the feminine ones, thereby indicating that the latter elicit emotions following a simpler underlying structure. Both results reflect larger issues in toy marketing, where gender essentialism is still dominant, and prompt further discussion and research.},
	address = {{London, UK}},
	author = {Marinelli, Luca and Saitis, Charalampos},
	booktitle = {9th European Conference on Media, Communication & Film},
	selected = {false},
	title = {Exploring the Dimensionality of the Affective Space Elicited by Gendered Toy Commercials},
	year = {2022}}

@inproceedings{charis_sempre_2021,
	abbr = {SEMPRE},
	abstract = {Timbre is defined as any auditory property other than pitch, duration, and loudness that allows two sounds to be distinguished. The Timbre Explorer (TE) is a synthesiser interface designed to demonstrate timbral dimensions of sound. This project aimed to develop and evaluate a web version of the TE that attempts to train its users and test their understanding of timbre as they go through a series of gamified tasks. A pilot study with 16 participants helped to identify shortcomings ahead of a full-sized study that will evaluate the performance of the TE as an educational aid and musical assessment tool.},
	address = {{Online}},
	author = {Saitis, Charalampos},
	booktitle = {Society for Education, Music, and Psychology Research Conference},
	selected = {false},
	title = {Development of a Web Application for the Education, Assessment, and Study of Timbre Perception},
	year = {2021}}

@inproceedings{vjosa_ic2s2_2021,
	abbr = {IC2S2},
	address = {{Online}},
	author = {Preniqi, Vjosa and Kalimeri, Kyriaki and Saitis, Charalampos},
	booktitle = {7th International Conference on Computational Social Science},
	poster = {IC2S2_Poster.pdf},
	selected = {false},
	title = {We are what we listen to: How moral values reflect on musical preferences},
	year = {2021}}

@inproceedings{russell_ijcnn_2022,
	abbr = {IJCNN},
	abstract = {This work investigates the application of deep learning to timbre transfer. The adopted approach combines Variational Autoencoders with Generative Adversarial Networks to construct meaningful representations of the source audio and produce realistic generations of the target audio and is applied to the Flickr 8k Audio dataset for transferring the vocal timbre between speakers and the URMP dataset for transferring the musical timbre between instruments. Variations of the adopted approach were trained, and performance was compared using the metrics SSIM (Structural Similarity Index) and FAD (Frechet Audio Distance). It was found that a many-to-many approach supersedes a one-to-one approach in terms of reconstructive capabilities, while one-to-one showed better results in terms of adversarial translation. The adoption of a basic over a bottleneck residual block design is more suitable for enriching content information about a latent space, and the decision on whether cyclic loss takes on a variational autoencoder or vanilla autoencoder approach does not have a significant impact on reconstructive and adversarial translation aspects of the model.},
	address = {{Padua, Italy}},
	author = {Sammut Bonnici, Russell and Benning, Martin and Saitis, Charalampos},
	booktitle = {International Joint Conference on Neural Networks},
	pdf = {paper_IJCNN_2022.pdf},
	poster = {poster_IJCNN_2022.pdf},
	selected = {false},
	title = {Timbre Transfer with Variational Auto Encoding and Cycle-Consistent Adversarial Networks},
	year = {2022}}

@article{hayes_disembodied_2022,
	abbr = {JAES},
	abstract = {Disembodied electronic sounds constitute a large part of the modern auditory lexicon, but research into timbre perception has focused mostly on the tones of conventional acoustic musical instruments. It is unclear whether insights from these studies generalise to electronic sounds, nor is it obvious how these relate to the creation of such sounds. In this work, we present an experiment on the semantic associations of sounds produced by FM synthesis with the aim of identifying whether existing models of timbre semantics are appropriate for such sounds. We applied a novel experimental paradigm in which experienced sound designers responded to semantic prompts by programming a synthesiser, and provided semantic ratings on the sounds they created. Exploratory factor analysis revealed a five-dimensional semantic space. The first two factors mapped well to the concepts of luminance, texture, and mass. The remaining three factors did not have clear parallels, but correlation analysis with acoustic descriptors suggested an acoustical relationship to luminance and texture. Our results suggest that further enquiry into the timbres of disembodied electronic sounds, their synthesis, and their semantic associations would be worthwhile, and that this could benefit research into auditory perception and cognition, as well as synthesis control and audio engineering.},
	author = {Hayes, Ben and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
	doi = {10.17743/jaes.2022.0006},
	issn = {15494950},
	journal = {Journal of the Audio Engineering Society},
	month = may,
	number = {5},
	pages = {373--391},
	pdf = {hayes_disembodied_2022.pdf},
	selected = {true},
	title = {Disembodied {{Timbres}}: {{A Study}} on {{Semantically Prompted FM Synthesis}}},
	volume = {70},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.17743/jaes.2022.0006}}

@inproceedings{hayes_neural_2021,
	abbr = {ISMIR},
	abstract = {We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully causal approach to neural audio synthesis which operates directly in the waveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU inference. The NEWT uses time-distributed multilayer perceptrons with periodic activations to implicitly learn nonlinear transfer functions that encode the characteristics of a target timbre. Once trained, a NEWT can produce complex timbral evolutions by simple affine transformations of its input and output signals. We paired the NEWT with a differentiable noise synthesiser and reverb and found it capable of generating realistic musical instrument performances with only 260k total model parameters, conditioned on F0 and loudness features. We compared our method to state-of-the-art benchmarks with a multi-stimulus listening test and the Fr{\'e}chet Audio Distance and found it performed competitively across the tested timbral domains. Our method significantly outperformed the benchmarks in terms of generation speed, and achieved real-time performance on a consumer CPU, both with and without FastNEWT, suggesting it is a viable basis for future creative sound design tools.},
	address = {{Online}},
	author = {Hayes, Ben and and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
	booktitle = {22nd International Society for Music Information Retrieval Conference},
	pdf = {hayes_neural_2021.pdf},
	selected = {true},
	title = {Neural Waveshaping Synthesis},
	year = {2021}}

@inproceedings{hayes_perceptual_2021,
	abbr = {ICMPC},
	abstract = {Electronic sound has a rich history, yet timbre research has typically focused on the sounds of physical instruments, while synthesised sound is often relegated to functional roles like recreating acoustic timbres. Studying the perception of synthesised sound can broaden our conception of timbre and improve musical synthesis tools. We aimed to identify the perceptually salient acoustic attributes of sounds produced by frequency modulation synthesis. We also aimed to test Zacharakis et al's luminance-texture-mass timbre semantic model [Music Perception, 31, 339--358 (2014)] in this domain. Finally, we aimed to identify effects of prior music or synthesis experience on these results. Our results suggest that discrimination of abstract electronic timbres may rely on attributes distinct from those used with acoustic timbres. Further, the most salient attributes vary with expertise. However, the use of semantic descriptors is similar to that of acoustic instruments, and is consistent across expertise levels.},
	address = {{Sheffield, UK}},
	author = {Hayes, Ben and and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
	booktitle = {16th International Conference on Music Perception and Cognition},
	pdf = {hayes_perceptual_2021.pdf},
	selected = {false},
	title = {Perceptual and semantic scaling of FM synthesis timbres: Common dimensions and the role of expertise},
	year = {2021}}

@inproceedings{hayes_perceptual_2020,
	abbr = {DMRN},
	abstract = {Many neural audio synthesis models learn a representational space which can be used for control or exploration of the sounds generated. It is unclear what relationship exists between this space and human perception of these sounds. In this work, we compute configurational similarity metrics between an embedding space learned by a neural audio synthesis model and conventional perceptual and seman- tic timbre spaces. These spaces are computed using abstract synthesised sounds. We find significant similarities between these spaces, suggesting a shared organisational influence.},
	address = {{London, UK}},
	author = {Hayes, Ben and Brosnahan, Luke and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
	booktitle = {{{DMRN}}+15: {{Digital Music Research Network One}}-Day {{Workshop}}},
	pdf = {hayes_perceptual_2020.pdf},
	selected = {false},
	title = {Perceptual {{Similarities}} in {{Neural Timbre Embeddings}}},
	year = {2020}}

@inproceedings{hayes_nash_2021,
	abbr = {DMRN},
	abstract = {The field of neural audio synthesis aims to produce audio using neural networks. A recent surge in its popularity has led to several high profile works achieving impressive feats of speech and music synthesis. The development of broadly accessible neural audio synthesis tools, conversely, has been limited, and creative applications of these technologies are mostly undertaken by those with technical know-how. Research has focused largely on tasks such as realistic speech and musical instrument synthesis, whereas investigations into high-level control, esoteric sound design capabilities, and interpretability have received less attention. To encourage innovative work addressing these gaps, C4DM's Special Interest Group on Neural Audio Synthesis (SIGNAS) propose to host our first Neural Audio Synthesis Hackathon: a two day event, with results to be presented in a session at DMRN+16.},
	address = {{London, UK}},
	author = {Hayes, Ben and Vahidi, Cyrus and Saitis, Charalampos},
	booktitle = {{{DMRN}}+16: {{Digital Music Research Network One}}-Day {{Workshop}}},
	pdf = {ben-DMRN-16.pdf},
	selected = {false},
	title = {NASH: the Neural Audio Synthesis Hackathon},
	year = {2021}}

@inproceedings{cyrus_dmrn_2021,
	abbr = {DMRN},
	abstract = {In this work, we outline initial steps towards modelling perceptual timbre dissimilarity. We use stimuli from 17 distinct subjective timbre studies and compute pairwise distances in the spaces of MFCCs, joint time-frequency scattering coefficients and Open-L3 embeddings. We analyze agreement of distances in these spaces with human dissimilarity ratings and highlight challenges of this task.},
	address = {{London, UK}},
	author = {Vahidi, Cyrus and Hayes, Ben and Saitis, Charalampos and Gy{\"o}rgy},
	booktitle = {{{DMRN}}+16: {{Digital Music Research Network One}}-Day {{Workshop}}},
	pdf = {cyrus-DMRN-16.pdf},
	selected = {false},
	title = {Acoustic Representations for Perceptual Timbre Similarity},
	year = {2021}}

@inproceedings{russell_dmrn_2021,
	abbr = {DMRN},
	abstract = {The combination of Variational Autoencoders (VAE) with Generative Adversarial Networks (GAN) motivates meaningful representations of audio in the context of timbre transfer. This was applied to different datasets for transferring vocal timbre between speakers and musical timbre between instruments. Variations of the approach were trained and generalised performance was compared using the Structural Similarity Index and Frechet Audio Distance. Many-to-many style transfer was found to improve reconstructive performance over one-to-one style transfer.},
	address = {{London, UK}},
	author = {Sammut Bonnici, Russell and Benning, Martin and Saitis, Charalampos},
	booktitle = {{{DMRN}}+16: {{Digital Music Research Network One}}-Day {{Workshop}}},
	pdf = {russell-DMRN-16.pdf},
	selected = {false},
	title = {Variational Auto Encoding and Cycle-Consistent Adversarial Networks for Timbre Transfer},
	year = {2021}}

@inproceedings{hayes_theres_2020,
	abbr = {Timbre},
	abstract = {Much previous research into timbre semantics (such as when an oboe is described as ``hollow'') has focused on sounds produced by acoustic instruments, particularly those associated with western tonal music (Saitis & Weinzierl, 2019). Many synthesisers are capable of producing sounds outside the timbral range of physical instruments, but which are still discriminable by their timbre. Research into the perception of such sounds, therefore, may help elucidate further the mechanisms underpinning our experience of timbre in the broader sense. In this paper, we present a novel paradigm on the application of semantic descriptors to sounds produced by experienced sound designers using an FM synthesiser with a full set of controls.},
	address = {{Thessaloniki, Greece (Online)}},
	author = {Hayes, Ben and Saitis, Charalampos},
	booktitle = {2nd {{International Conference}} on {{Timbre}}},
	pdf = {hayes_theres_2020.pdf},
	selected = {false},
	title = {There's More to Timbre than Musical Instruments: Semantic Dimensions of {{FM}} Sounds},
	year = {2020}}

@inproceedings{zacharakis_evidence_2020,
	abbr = {Timbre},
	abstract = {Research on timbre perception is typically conducted under controlled laboratory conditions where every effort is made to maintain stimulus presentation conditions fixed (McAdams, 2019). This conforms with the ANSI (1973) definition of timbre suggesting that in order to judge the timbre differences between a pair of sounds the rest perceptual attributes (i.e., pitch, duration and loudness) should remain unchanged. Therefore, especially in pairwise dissimilarity studies, particular care is taken to ensure that loudness is not used by participants as a criterion for judgements by equalising it across experimental stimuli. On the other hand, conducting online experiments is an increasingly favoured practice in the music perception and cognition field as targeting relevant communities can potentially provide a large number of suitable participants with relatively little time investment from the side of the experimenters (e.g., Woods et al., 2015). However, the strict requirements for stimuli preparation and presentation prevents timbre studies from conducting online experimentation. Despite the obvious difficulties in imposing equal loudness on online experiments, the different playback equipment chain (DACs, pre-amplifiers, headphones) will also almost inevitably `colour' the sonic outcome in a different way. Despite the above limitations, in a social distancing time like this, it would be of major importance to be able to lift some of the physical requirements in order to carry on conducting behavioural research on timbre perception. Therefore, this study aims to investigate the extent to which an uncontrolled online replication of a past laboratory-conducted pairwise dissimilarity task will distort the findings.},
	address = {{Thessaloniki, Greece (Online)}},
	author = {Zacharakis, Asterios and Hayes, Ben and Saitis, Charalampos and Pastiadis, Konstantinos},
	booktitle = {2nd {{International Conference}} on {{Timbre}}},
	pdf = {zacharakis_evidence_2020.pdf},
	selected = {false},
	title = {Evidence for Timbre Space Robustness to an Uncontrolled Online Stimulus Presentation},
	year = {2020}}

@inproceedings{delgado_vocimit_2020,
	abbr = {Timbre},
	abstract = {The imitation of non-vocal sounds using the human voice is a resource we sometimes rely on when communicating sound concepts to other people. Query by Vocal Percussion (QVP) is a subfield in Music Information Retrieval (MIR) that explores techniques to query percussive sounds using vocal imitations as input, usually plosive consonant sounds. The goal of this work was to investigate timbral relationships between real drum sounds and their vocal imitations. We believe these insights could shed light on how to select timbre descriptors for extraction when designing offline and online QVP systems. In particular, we studied a dataset composed of 30 acoustic and electronic drum sound recordings and vocal imitations of each sound performed by 14 musicians. Our approach was to study the correlation of audio content descriptors of timbre extracted from the drum samples with the same descriptors taken from vocal imitations. Three timbral descriptors were selected: the Log Attack Time (LAT), the Spectral Centroid (SC), and the Derivative After Maximum of the sound envelope (DAM). LAT and SC have been shown to represent salient dimensions of timbre across different types of sounds including percussion. In this sense, one intriguing question would be to what extent listeners can communicate these salient timbral cues in vocal imitations. The third descriptor, DAM, was selected for its role in describing the sound's tail, which we considered to be a relevant part of percussive utterances.},
	address = {{Thessaloniki, Greece (Online)}},
	author = {Delgado, Alejandro and Saitis, Charalampos and Sandler, Mark},
	booktitle = {2nd {{International Conference}} on {{Timbre}}},
	pdf = {delgado_timbre_2020.pdf},
	selected = {false},
	title = {Spectral and Temporal Timbral Cues of Vocal Imitations},
	year = {2020}}

@inproceedings{vahidi_timbre_2020,
	abbr = {Timbre},
	abstract = {In this study, we produce a geometrically scaled perceptual timbre space from dissimilarity ratings of subtractive synthesized sounds and correlate the resulting dimensions with a set of acoustic descriptors. We curate a set of 15 sounds, produced by a synthesis model that uses varying source waveforms, frequency modulation (FM) and a lowpass filter with an enveloped cutoff frequency. Pairwise dissimilarity ratings were collected within an online browser-based experiment. We hypothesized that a varied waveform input source and enveloped filter would act as the main vehicles for timbral variation, providing novel acoustic correlates for the perception of synthesized timbres.},
	address = {{Thessaloniki, Greece (Online)}},
	author = {Vahidi, Cyrus and Fazekas, Gy{\"o}rgy and Saitis, Charalampos and Palladini, Alessandro},
	booktitle = {2nd {{International Conference}} on {{Timbre}}},
	pdf = {https://arxiv.org/pdf/2009.11706.pdf},
	selected = {false},
	title = {Timbre Space Representation of a Subtractive Synthesizer},
	year = {2020}}

@inproceedings{drouzas_timbre_2020,
	abbr = {Timbre},
	abstract = {Amongst the most common descriptive expressions of timbre used by musicians, music engineers, audio researchers as well as everyday listeners are words related to the notion of brightness (e.g., bright, dark, dull, brilliant, shining). From a psychoacoustic perspective, brightness ratings of instrumental timbres as well as music excerpts systematically correlate with the centre of gravity of the spectral envelope and thus brightness as a semantic descriptor of musical sound has come to denote a prevalence of high-frequency over low-frequency energy. However, relatively little is known about the higher-level cognitive processes underpinning musical brightness ratings. Psycholinguistic investigations of verbal descriptions of timbre suggest a more complex, polysemic picture (Saitis & Weinzierl 2019) that warrants further research. To better understand how musical brightness is conceptualised by listeners, here we analysed free verbal descriptions collected along brightness ratings of short music snippets (involving 69 listeners) and brightness ratings of orchestral instrument notes (involving 68 listeners). Such knowledge can help delineate the intrinsic structure of brightness as a perceptual attribute of musical sounds, and has broad implications and applications in orchestration, audio engineering, and music psychology.},
	address = {{Thessaloniki, Greece (Online)}},
	author = {Drouzas, Christos and Saitis, Charalampos},
	booktitle = {2nd {{International Conference}} on {{Timbre}}},
	pdf = {https://timbre2020.mus.auth.gr/assets/papers/9.Drouzas.pdf},
	selected = {false},
	title = {Verbal description of musical brightness},
	year = {2020}}

@article{charis_JASA_2020,
	abbr = {JASA},
	abstract = {Timbre dissimilarity of orchestral sounds is well-known to be multidimensional, with attack time and spectral centroid representing its two most robust acoustical correlates. The centroid dimension is traditionally considered as reflecting timbral brightness. However, the question of whether multiple continuous acoustical and/or categorical cues influence brightness perception has not been addressed comprehensively. A triangulation approach was used to examine the dimensionality of timbral brightness, its robustness across different psychoacoustical contexts, and relation to perception of the sounds' source-cause. Listeners compared 14 acoustic instrument sounds in three distinct tasks that collected general dissimilarity, brightness dissimilarity, and direct multi-stimulus brightness ratings. Results confirmed that brightness is a robust unitary auditory dimension, with direct ratings recovering the centroid dimension of general dissimilarity. When a two-dimensional space of brightness dissimilarity was considered, its second dimension correlated with the attack-time dimension of general dissimilarity, which was interpreted as reflecting a potential infiltration of the latter into brightness dissimilarity. Dissimilarity data were further modeled using partial least-squares regression with audio descriptors as predictors. Adding predictors derived from instrument family and the type of resonator and excitation did not improve the model fit, indicating that brightness perception is underpinned primarily by acoustical rather than source-cause cues.},
	author = {Saitis, Charalampos and Siedenburg, Kai},
	journal = {The Journal of the Acoustical Society of America},
	number = {4},
	pages = {2256--2266},
	pdf = {2256_1_final_published.pdf},
	publisher = {Acoustical Society of America},
	selected = {true},
	title = {Brightness perception for musical instrument sounds: Relation to timbre dissimilarity and source-cause categories},
	volume = {148},
	year = {2020}}

@inproceedings{IJCNN_2021_Accepted,
	abbr = {IJCNN},
	abstract = {Convolutional Neural Networks have been extensively explored in the task of automatic music tagging. The problem can be approached by using either engineered time-frequency features or raw audio as input. Modulation filter bank representations that have been actively researched as a basis for timbre perception have the potential to facilitate the extraction of perceptually salient features. We explore end-to-end learned front-ends for audio representation learning, ModNet and SincModNet, that incorporate a temporal modulation processing block. The structure is effectively analogous to a modulation filter bank, where the FIR filter center frequencies are learned in a data-driven manner. The expectation is that a perceptually motivated filter bank can provide a useful representation for identifying music features. Our experimental results provide a fully visualisable and interpretable front-end temporal modulation decomposition of raw audio. We evaluate the performance of our model against the state-of-the-art of music tagging on the MagnaTagATune dataset. We analyse the impact on performance for particular tags when time-frequency bands are subsampled by the modulation filters at a progressively reduced rate. We demonstrate that modulation filtering provides promising results for music tagging and feature representation, without using extensive musical domain knowledge in the design of this frontend.},
	author = {Vahidi, Cyrus and Saitis, Charalampos and Fazekas, Gy{\"o}rgy},
	booktitle = {International Joint Conference on Neural Networks},
	pages = {1--7},
	pdf = {IJCNN_2021_Accepted.pdf},
	publisher = {IEEE},
	selected = {false},
	title = {A Modulation Front-End for Music Audio Tagging},
	year = {2021}}

@inproceedings{Alejandro_AES151,
	abbr = {AES},
	abstract = {Vocal Percussion Transcription (VPT) aims at detecting vocal percussion sound events in a beatboxing performance and classifying them into the correct drum instrument class (kick, snare, or hi-hat). To do this in an online (real-time) setting, however, algorithms are forced to classify these events within just a few milliseconds after they are detected. The purpose of this study was to investigate which phoneme-to-instrument mappings are the most robust for online transcription purposes. We used three different evaluation criteria to base our decision upon: frequency of use of phonemes among different performers, spectral similarity to reference drum sounds, and classification separability. With these criteria applied, the recommended mappings would potentially feel natural for performers to articulate while enabling the classification algorithms to achieve the best performance possible. Given the final results, we provided a detailed discussion on which phonemes to choose given different contexts and applications.},
	author = {Delgado, Alejandro and Saitis, Charalampos and Sandler, Mark},
	booktitle = {151st Audio Engineering Society Convention},
	pdf = {Alejandro_AES151.pdf},
	publisher = {Audio Engineering Society},
	selected = {false},
	title = {Phoneme Mappings for Online Vocal Percussion Transcription},
	note = {Honourable Mention for Outstanding Paper},
	year = {2021}}

@inproceedings{weber2020towards,
	abbr = {HAID},
	abstract = {To enable a transition towards rich vibrotactile feedback in applications and media content, a complete end-to-end system --- from the design of the tactile experience all the way to the tactile stimulus reproduction --- needs to be considered. Currently, most applications are at best limited to dull vibration patterns due to limited hard- and software implementations, while the design of ubiquitous platform-agnostic tactile stimuli remains challenging due to a lack of standardized protocols and tools for tactile design, storage, transport, and reproduction. This work proposes a conceptual framework, utilizing audio assets as a starting point for the design of vibrotactile stimuli, including ideas for a parametric tactile data model, and outlines challenges for a platform-agnostic stimuli reproduction. Finally, the benefits and shortcomings of a commercial and wide-spread vibrotactile API are investigated as an example for the current state of a complete end-to-end framework.},
	author = {Weber, Maximilian and Saitis, Charalampos},
	booktitle = {10th International Workshop on Haptic and Audio Interaction Design},
	pdf = {https://hal.archives-ouvertes.fr/hal-02901209/document},
	title = {Towards a framework for ubiquitous audio-tactile design},
	year = {2020}}

@inproceedings{luca_SMC_2020,
	abbr = {SMC},
	abstract = {To investigate variations in the timbre space with regards to musical dynamics, convolutional neural networks (CNNs) were trained on modulation power spectra (MPS), melscaled and ERB-scaled spectrograms of single notes of sustained instruments played at two dynamics extremes (pp and ff). The samples, from an extensive dataset of several timbre families, were rms normalized in order to eliminate the loudness information and force the network to focus on timbre attributes of musical dynamics that are shared across different instrument families. The proposed CNN architecture obtained competitive results in three classification tasks with all three input representations. In order to compare the different input representations, the test sets in three experiments were partitioned in order to promote or avoid selection bias. When selection bias was avoided, models trained on MPS were outperformed by those trained on time-frequency representations, conversely, those trained on MPS achieved the best results when selection bias was promoted. Low-temporal modulations emerged in classspecific MPS saliency maps as markers of musical dynamics. This led to the implementation of a MPS-based scalar descriptor of timbre that largely outperformed the chosen baseline (44.8{\%} error reduction).},
	address = {Online},
	author = {Marinelli, Luca and Lykartsis, Athanasios and Weinzierl, Stefan and Saitis, Charalampos},
	booktitle = {17th Sound and Music Computing Conference},
	pdf = {https://www.researchgate.net/profile/Charalampos-Saitis/publication/343775001_MUSICAL_DYNAMICS_CLASSIFICATION_WITH_CNN_AND_MODULATION_SPECTRA/links/5f3ee47392851cd3020dac6e/MUSICAL-DYNAMICS-CLASSIFICATION-WITH-CNN-AND-MODULATION-SPECTRA.pdf},
	selected = {false},
	title = {Musical dynamics classification with CNN and modulation spectra},
	year = {2020}}

@inproceedings{Alejandro_SMC_2022,
	abbr = {SMC},
	abstract = {Vocal Percussion Transcription (VPT) is concerned with the automatic detection and classification of vocal percussion sound events, allowing music creators and producers to sketch drum lines on the fly among others. VPT classifiers usually learn best from small user-specific datasets, which usually restrict modelling to small input feature sets to avoid model overfitting. This study explores several deep supervised learning strategies to obtain informative feature sets for amateur VPT classification. We evaluated their performance on regular VPT classification tasks and compared them with several baseline approaches including feature selection methods and a state-of-the-art speech recognition engine. These proposed learning models were supervised with several label sets containing information from four different levels of abstraction: instrument-level, syllable-level, phoneme-level, and boxeme-level. Results suggest that convolutional neural networks supervised with syllable-level annotations produced the most informative embeddings for VPT systems, which can be used as input representations to fit classifiers with. Finally, we used back-propagation-based saliency maps to investigate the importance of difference spectrogram regions for feature learning.},
	address = {Saint-Etienne, France},
	arxiv = {2204.04646},
	author = {Delgado, Alejandro and Demirel, Emir, and Subramanian, Vinod and Saitis, Charalampos and Sandler, Mark},
	booktitle = {19th Sound and Music Computing Conference},
	pdf = {https://zenodo.org/record/6797666#.YyhGDuxKjOQ},
	selected = {false},
	title = {Deep Embeddings for Robust User-Based Amateur Vocal Percussion Transcription},
	year = {2022}}

@inproceedings{Alejandro_ICMC_2021,
	abbr = {ICMC},
	abstract = {The imitation of percussive sounds via the human voice is a natural and effective tool for communicating rhythmic ideas on the fly. Thus, the automatic retrieval of drum sounds using vocal percussion can help artists prototype drum patterns in a comfortable and quick way, smoothing the creative workflow as a result. Here we explore different strategies to perform this type of query, making use of both traditional machine learning algorithms and recent deep learning techniques. The main hyperparameters from the models involved are carefully selected by feeding performance metrics to a grid search algorithm. We also look into several audio data augmentation techniques, which can potentially regularise deep learning models and improve generalisation. We compare the final performances in terms of effectiveness (classification accuracy), efficiency (computational speed), stability (performance consistency), and interpretability (decision patterns), and discuss the relevance of these results when it comes to the design of successful query-by-vocal-percussion systems.},
	address = {Online},
	arxiv = {2110.09223},
	author = {Delgado, Alejandro and McDonald SKoT, and Xu, Ning and Saitis, Charalampos and Sandler, Mark},
	booktitle = {46th International Computer Music Conference},
	selected = {false},
	title = {Learning Models for Query by Vocal Percussion: A Comparative Study},
	year = {2021}}

@inproceedings{Lam_NIME21,
	abbr = {NIME},
	abstract = {When two sounds are played at the same loudness, pitch, and duration, what sets them apart are their timbres. This study documents the design and implementation of the Timbre Explorer, a synthesizer interface based on efforts to dimensionalize this perceptual concept. The resulting prototype controls four perceptually salient dimensions of timbre in real-time: attack time, brightness, spectral flux, and spectral density. A graphical user interface supports user understanding with live visualizations of the effects of each dimension. The applications of this interface are three-fold; further perceptual timbre studies, usage as a practical shortcut for synthesizers, and educating users about the frequency domain, sound synthesis, and the concept of timbre. The project has since been expanded to a standalone version independent of a computer and a purely online web-audio version.},
	author = {Lam, Joshua Ryan and Saitis, Charalampos},
	booktitle = {International Conference on New Interfaces for Musical Expression},
	link = {https://doi.org/10.21428/92fbeb44.92a95683},
	organization = {PubPub},
	selected = {false},
	title = {The Timbre Explorer: A Synthesizer Interface for Educational Purposes and Perceptual Studies},
	year = {2021}}

@incollection{Saitis_chap5,
	abbr = {Chapter},
	abstract = {Because humans lack a sensory vocabulary for auditory experiences, timbral qualities of sounds are often conceptualized and communicated through readily available sensory attributes from different modalities (e.g., bright, warm, sweet) but also through the use of onomatopoeic attributes (e.g., ringing, buzzing, shrill) or nonsensory attributes relating to abstract constructs (e.g., rich, complex, harsh). The analysis of the linguistic description of timbre, or timbre semantics, can be considered as one way to study its perceptual representation empirically. In the most commonly adopted approach, timbre is considered as a set of verbally defined perceptual attributes that represent the dimensions of a semantic timbre space. Previous studies have identified three salient semantic dimensions for timbre along with related acoustic properties. Comparisons with similarity-based multidimensional models confirm the strong link between perceiving timbre and talking about it. Still, the cognitive and neural mechanisms of timbre semantics remain largely unknown and underexplored, especially when one looks beyond the case of acoustic musical instruments.},
	address = {Cham},
	author = {Saitis, Charalampos and Weinzierl, Stefan},
	booktitle = {{Timbre: Acoustics, Perception, and Cognition}},
	pages = {119--149},
	pdf = {Saitis_chap5.pdf},
	publisher = {Springer Handbook of Auditory Research 69},
	selected = {false},
	title = {The Semantics of Timbre},
	year = {2019}}

@incollection{Timbre_chapter1,
	abbr = {Chapter},
	abstract = {Timbre is a foundational aspect of hearing. The remarkable ability of humans to recognize sound sources and events (e.g., glass breaking, a friend's voice, a tone from a piano) stems primarily from a capacity to perceive and process differences in the timbre of sounds. Roughly defined, timbre is thought of as any property other than pitch, duration, and loudness that allows two sounds to be distinguished. Current research unfolds along three main fronts: (1) principal perceptual and cognitive processes; (2) the role of timbre in human voice perception, perception through cochlear implants, music perception, sound quality, and sound design; and (3) computational acoustic modeling. Along these three scientific fronts, significant breakthroughs have been achieved during the decade prior to the production of this volume. Bringing together leading experts from around the world, this volume provides a joint forum for novel insights and the first comprehensive modern account of research topics and methods on the perception, cognition, and acoustic modeling of timbre. This chapter provides background information and a roadmap for the volume.},
	author = {Siedenburg, Kai and Saitis, Charalampos and McAdams, Stephen},
	booktitle = {{Timbre: Acoustics, Perception, and Cognition}},
	pages = {1--19},
	pdf = {Timbre_chapter1.pdf},
	publisher = {Springer Handbook of Auditory Research 69},
	selected = {false},
	title = {The present, past, and future of timbre research},
	year = {2019}}

@incollection{Timbre_chapter11,
	abbr = {Chapter},
	abstract = {This chapter introduces acoustic modeling of timbre with the audio descriptors commonly used in music, speech, and environmental sound studies. These descriptors derive from different representations of sound, ranging from the waveform to sophisticated time-frequency transforms. Each representation is more appropriate for a specific aspect of sound description that is dependent on the information captured. Auditory models of both temporal and spectral information can be related to aspects of timbre perception, whereas the excitation-filter model of sound production provides links to the acoustics of sound production. A brief review of the most common representations of audio signals used to extract audio descriptors related to timbre is followed by a discussion of the audio descriptor extraction process using those representations. This chapter covers traditional temporal and spectral descriptors, including harmonic description, time-varying descriptors, and techniques for descriptor selection and descriptor decomposition. The discussion is focused on conceptual aspects of the acoustic modeling of timbre and the relationship between the descriptors and timbre perception, semantics, and cognition, including illustrative examples. The applications covered in this chapter range from timbre psychoacoustics and multimedia descriptions to computer-aided orchestration and sound morphing. Finally, the chapter concludes with speculation on the role of deep learning in the future of timbre description and on the challenges of audio content descriptors of timbre.},
	author = {Caetano, Marcelo and Saitis, Charalampos and Siedenburg, Kai},
	booktitle = {{Timbre: Acoustics, Perception, and Cognition}},
	pages = {1--19},
	pdf = {Caetano_chap11.pdf},
	publisher = {Springer Handbook of Auditory Research 69},
	selected = {false},
	title = {Audio Content Descriptors of Timbre},
	year = {2019}}

@book{Timbre_book,
	abbr = {Book},
	author = {Siedenburg, Kai and Saitis, Charalampos and McAdams, Stephen and Popper, Arthur N. and Fay, Richard R.},
	link = {https://link.springer.com/book/10.1007/978-3-030-14832-4},
	publisher = {Springer Handbook of Auditory Research 69},
	selected = {false},
	title = {{Timbre: Acoustics, Perception, and Cognition}},
	year = {2019}}

@inproceedings{Saitis_SMPC_22,
	abbr = {SMPC},
	abstract = {Brightness is among the most studied aspects of timbre perception. Psychoacoustically, sounds described as ''bright'' vs ''dark'' typically exhibit a high vs low frequency emphasis in the spectrum. However, relatively little is known about the neurocognitive mechanisms that facilitate these ``metaphors we listen with.'' Do they originate in universal mental representations common to more than one sensory modality? Triangulating three different interaction paradigms, we investigated using speeded identification whether unimodal and crossmodal interference occurs when timbral brightness, as modelled by the centroid of the spectral envelope, and 1) pitch height, 2) visual brightness, 3) numerical value processing are semantically incongruent. In three online pilot tasks, 58 participants were presented a baseline stimulus (a pitch, gray square, or numeral) then asked to quickly identify a target stimulus that is higher/lower, brighter/darker, or greater/less than the baseline, respectively, after being primed with a bright or dark synthetic harmonic tone. Additionally, in the pitch and visual tasks, a deceptive same-target condition was included. Results suggest that timbral brightness modulates the perception of pitch and visual brightness, but not numerical value. Semantically incongruent pitch height-timbral brightness shifts produced significantly slower choice reaction time and higher error compared to congruent pairs; timbral brightness also had a strong biasing effect in the same-target condition (i.e., people heard the same pitch as higher when the target tone was timbrally brighter than the baseline, and vice versa with darker tones). In the visual task, incongruent pairings of gray squares and tones elicited slower choice reaction times than congruent pairings. No interference was observed in the number comparison task. We are currently following up on these results with a larger online replication sample, and an fMRI study to investigate the relevant neural mechanisms. Our findings shed light on the multisensory nature of experiencing timbre.},
	author = {Saitis, Charalampos, and Wallmark, Zachary and Liu, Annie},
	booktitle = {Biennial Meeting of the Society for Music Perception and Cognition},
	poster = {Saitis_SMPC_22.pdf},
	selected = {false},
	title = {Auditory brightness perception investigated by unimodal and crossmodal interference},
	year = {2022}}

@book{Timbre_2020_proc,
	abbr = {Timbre},
	author = {Zacharakis, Asterios and Saitis, Charalampos and Siedenburg, Kai},
	link = {http://timbre2020.mus.auth.gr/assets/papers/Timbre2020_proceedings.pdf},
	publisher = {The School of Music Studies, Aristotle University of Thessaloniki},
	selected = {false},
	title = {{Proceedings of the 2nd International Conference on Timbre}},
	year = {2020}}

@book{haid_2022_proc,
	abbr = {HAID},
	author = {Saitis, Charalampos and Farkhatdinov, Ildar and Papetti, Stefano},
	link = {https://link.springer.com/book/10.1007/978-3-031-15019-7},
	publisher = {Springer Lecture Notes in Computer Science 13417},
	selected = {false},
	title = {{Proceedings of the 11th International Workshop on Haptic and Audio Interaction Design}},
	year = {2022}}
